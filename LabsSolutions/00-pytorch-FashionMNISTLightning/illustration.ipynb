{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b43f6a5",
   "metadata": {},
   "source": [
    "# Fashion MNIST with pytorch lightning and neptune.ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86389472",
   "metadata": {},
   "source": [
    "This notebook is accompanying the code on the [repository](https://github.com/jeremyfix/deeplearning-lectures/tree/lightning/LabsSolutions/00-pytorch-FashionMNISTLightning). This is a tutorial for illustrating deep learning training with [pytorch](www.pytorch.org), more specifically the higher level framework [pytorch lightning](https://www.pytorchlightning.ai/). We monitor the training using [neptune.ai](https://app.neptune.ai/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f8024",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup up the environment\n",
    "\n",
    "# For CPU only\n",
    "#!pip install -r requirements.txt\n",
    "\n",
    "# For GPU \n",
    "!pip install -r requirements_cuda.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e627a9",
   "metadata": {},
   "source": [
    "## Dataloader illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0112e3f",
   "metadata": {},
   "source": [
    "The first step of every deep learning script is to deal with the dataloading. This means writing a [dataset object](https://pytorch.org/docs/stable/data.html#dataset-types) or using already programmed dataset objects and plugging it into a dataloader. A dataloader allows to iterate over the dataset, providing a sequence of mini-batches of data. As we are dealing with images, our dataloader is providing 4D tensor (B, C, H, W) following the pytorh convention of Batch, Channel, Height, Width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f7ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fa20c6",
   "metadata": {},
   "source": [
    "We first build our dataloaders to see the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf91e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_threads = 4\n",
    "valid_ratio = 0.2\n",
    "batch_size = 128\n",
    "classes_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal','Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "loaders, fnorm = data.make_dataloaders(valid_ratio,\n",
    "                                  batch_size,\n",
    "                                  num_threads,\n",
    "                                  normalize=False,\n",
    "                                  dataaugment_train=False,\n",
    "                                  dataset_dir=None,\n",
    "                                  normalizing_tensor_path=None)\n",
    "train_loader, valid_loader, test_loader = loaders\n",
    "\n",
    "data.display_samples(train_loader, 10, 'fashionMNIST_samples.png', classes_names)\n",
    "\n",
    "X, y = next(iter(train_loader))\n",
    "print(f\"A minibatch of data is a tensor X of shape {X.shape} and y of shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f773ddd6",
   "metadata": {},
   "source": [
    "And we now play around with data augmentation. Data augmentation is a very important technique for regularization, i.e. generating many variation of your data for which you can also compute the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3136d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders, fnorm = data.make_dataloaders(valid_ratio,\n",
    "                                  batch_size,\n",
    "                                  num_threads,\n",
    "                                  False,\n",
    "                                  dataaugment_train=True,\n",
    "                                  dataset_dir=None,\n",
    "                                  normalizing_tensor_path=None)\n",
    "# Let us take the first sample of the dataset and sample it several\n",
    "# times \n",
    "train_loader, _, _ = loaders\n",
    "sample_idx = random.randint(0, len(train_loader.dataset))\n",
    "samples = [train_loader.dataset[sample_idx][0] for i in range(10)]\n",
    "label = train_loader.dataset[sample_idx][1] \n",
    "\n",
    "# Build a torch tensor from the list of samples\n",
    "samples = torch.cat(samples, dim=0).unsqueeze(dim=1) # to add C=1\n",
    "\n",
    "data.display_tensor_samples(samples, label,\n",
    "                       'fashionMNIST_sample_aug.png',\n",
    "                        classes_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee18871",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b830ec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f3b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model\": \"linear\",\n",
    "    \"num_workers\": 2,\n",
    "    \"normalize\": True,\n",
    "    \"data_augment\": True,\n",
    "    \"dataset_dir\": None,\n",
    "    \"weight_decay\": 0.0    \n",
    "}\n",
    "\n",
    "train.main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
